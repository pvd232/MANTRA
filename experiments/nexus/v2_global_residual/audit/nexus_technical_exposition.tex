\documentclass[11pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath, amssymb, amsfonts}
\usepackage{geometry}
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage{xcolor}
\usepackage{listings}

\geometry{margin=1in}

\title{MANTRA: Multi-modal Architecture for Network-based Transcriptional Response Analysis \\ \large Technical Exposition of the Nexus Integration}
\author{Antigravity Agentic Systems}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
This document provides a comprehensive technical overview of the MANTRA project, with a specific focus on the integration of the Nexus Global Reservoir module. We detail the architectural shift from episodic episodic memory to persistent global manifolds, the resolution of the ``Double Counting'' and ``Index Cheating'' pathologies, and the achievement of recursive additive gains in program-space delta predictions.
\end{abstract}

\section{The MANTRA Objective}
The fundamental goal of MANTRA is to model the high-dimensional response of a cellular system to external perturbations. Given a regulator $R$ and a dose $D$, we seek to predict the vector of gene expression changes $\Delta E \in \mathbb{R}^G$, where $G \approx 20,000$.

The problem is structured as a cascade of inferential stages:
\begin{enumerate}
    \item \textbf{Graph-Prior Inference}: Using a GNN ($f_\theta$) to propagate signals through the gene regulatory network to predict $\Delta E$.
    \item \textbf{Latent Program Decomposition}: Mapping $\Delta E$ to a compressed program space $\Delta P \in \mathbb{R}^K$ using cNMF consensus loadings.
    \item \textbf{Residual Memory Correction}: Using Nexus to provide additive corrections ($\Delta P_{corr}$) to the program activations.
    \item \textbf{Downstream Trait Readout}: Mapping the stabilized $\Delta P$ to trait expression deltas $\Delta T$ via SMR-derived weights.
\end{enumerate}

\section{Mathematical Framework}

\subsection{The GNN Baseline}
The GNN predicts a coarse latent activation vector $a_{GNN} \in \mathbb{R}^K$:
\begin{equation}
    a_{GNN} = \text{GNN}(R, D | \mathcal{G})
\end{equation}
where $\mathcal{G}$ is the prior gene interaction graph.

\subsection{The Nexus Global Reservoir (V2)}
Nexus functions as a global residual corrector within the program space. Unlike NLP transformers that use episodic memory, Nexus V2 utilizes a \textbf{State-Persistent Manifold} (CAM).

The stabilized program activations $\Delta P_{final}$ are calculated as:
\begin{equation}
    \Delta P_{final} = (\Delta E_{gnn} \cdot W_{cnmf}) + \text{Retrieve}(\text{Embed}(R, D), \mathcal{M})
\end{equation}

The training objective for Nexus is the \textbf{Residual Loss}:
\begin{equation}
    \mathcal{L}_{Nexus} = \| \Delta P_{obs} - \Delta P_{final} \|^2
\end{equation}

\subsection{The Hybrid Retrieval Mechanism}
Retrieval in Nexus V2 is partitioned into a two-stage process:

\subsubsection{Lexical Bucketing (Hash)}
The input regulator index $I_R$ is mapped to a manifold bucket $\mathcal{B}$ via:
\begin{equation}
    \mathcal{B} = I_R \pmod{N_{buckets}}
\end{equation}
This deterministic hash restricts the search space to a sparse subset of memory slots, ensuring that the model maintains specific high-fidelity records for individual perturbations.

\subsubsection{Semantic Alignment (Search)}
Within the selected bucket, the model generates a \textbf{Unified Query} $Q$:
\begin{equation}
    Q = \text{Normalize}(\alpha \cdot H_{semantic} + (1-\alpha) \cdot C_{lexical})
\end{equation}
where $H$ is the hidden state representing the CURRENT cell state context, and $C$ is the learned centroid anchor for that bucket. The final retrieval is a weighted average of the slots in $\mathcal{B}$, prioritized by their cosine similarity to $Q$.

\subsection{The Honest Indexing Protocol}
To prevent ``causality leaks'' (Index Cheating), we enforce a strict retrieval boundary. The input sequence is formatted as:
\begin{equation}
    [\text{REG}] \, [\text{DOSE}] \, [\text{STATE}] \, | \, [\text{PROGRAM\_CONTENT}]
\end{equation}
The prediction is derived exclusively from the hidden state at the \texttt{[STATE]} token (Index 2). This ensures that the model cannot use self-attention to ``peek'' at the program content tokens during training, forcing it to rely purely on the Centroid-Addressable Manifold.

\section{Empirical Results}
The transition from Naive Integration (V1) to the Global Residual Cache (V2) resulted in a performance flip from destructive to constructive.

\begin{table}[h]
\centering
\begin{tabular}{@{}lll@{}}
\toprule
Metric & MANTRA Baseline & MANTRA + Nexus (V2) \\ \midrule
Program-Space MSE & 0.7525 & 0.7449 \\
Additive Improvement & - & \textbf{+1.02\%} \\
Train-Bound Recall & - & ~77\% \\ \bottomrule
\end{tabular}
\caption{Comparison of Baseline vs Nexus V2 Correction.}
\end{table}

\section{Conclusion}
Nexus V2 transforms the MANTRA pipeline from a purely predictive model into a hybrid \textbf{Predicate-Signature System}. By offloading the memorization of rare perturbation signatures to the Global Manifold, we allow the GNN to focus on generalized biological rules while maintaining high-fidelity accuracy on known exceptions.

\end{document}
